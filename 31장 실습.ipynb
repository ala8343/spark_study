{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-17-134.ec2.internal:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-74134b9a3676>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-74134b9a3676>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    curl -O http://download.tensorflow.org/example_images/flower_photos.tgz\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# databricks 런타임 환경에서 테스트 하기 위해서는 아래의 코드를 먼저 실행해야 합니다.\n",
    "%sh\n",
    "curl -O http://download.tensorflow.org/example_images/flower_photos.tgz\n",
    "tar xzf flower_photos.tgz &>/dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.fs.ls('file:/databricks/driver/flower_photos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = '/tmp/flower_photos'\n",
    "dbutils.fs.mkdirs(img_dir)\n",
    "\n",
    "dbutils.fs.cp('file:/databricks/driver/flower_photos/tulips', img_dir + \"/tulips\", recurse=True)\n",
    "dbutils.fs.cp('file:/databricks/driver/flower_photos/daisy', img_dir + \"/daisy\", recurse=True)\n",
    "dbutils.fs.cp('file:/databricks/driver/flower_photos/LICENSE.txt', img_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_img_dir = img_dir + \"/sample\"\n",
    "dbutils.fs.rm(sample_img_dir, recurse=True)\n",
    "dbutils.fs.mkdirs(sample_img_dir)\n",
    "files =  dbutils.fs.ls(img_dir + \"/daisy\")[0:10] + dbutils.fs.ls(img_dir + \"/tulips\")[0:1]\n",
    "for f in files:\n",
    "  dbutils.fs.cp(f.path, sample_img_dir)\n",
    "\n",
    "dbutils.fs.ls(sample_img_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.image import ImageSchema\n",
    "\n",
    "# 이미지 파일이 많기 때문에 /tulips 디렉터리와 /daisy 디렉터리의 일부 파일을 /sample 디렉터리에 복제하여 사용합니다.\n",
    "# 약 10개의 파일을 /sample 디렉터리에 복제합니다.\n",
    "img_dir = '/data/deep-learning-images/'\n",
    "sample_img_dir = img_dir + \"/sample\"\n",
    "\n",
    "image_df = ImageSchema.readImages(sample_img_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.image import ImageSchema\n",
    "from pyspark.sql.functions import lit\n",
    "from sparkdl.image import imageIO\n",
    "\n",
    "tulips_df = ImageSchema.readImages(img_dir + \"/tulips\").withColumn(\"label\", lit(1))\n",
    "daisy_df = imageIO.readImagesWithCustomFn(img_dir + \"/daisy\", decode_f=imageIO.PIL_decode).withColumn(\"label\", lit(0))\n",
    "tulips_train, tulips_test = tulips_df.randomSplit([0.6, 0.4])\n",
    "daisy_train, daisy_test = daisy_df.randomSplit([0.6, 0.4])\n",
    "train_df = tulips_train.unionAll(daisy_train)\n",
    "test_df = tulips_test.unionAll(daisy_test)\n",
    "\n",
    "# 메모리 오버헤드를 줄이기 위해 파티션을 나눕니다.\n",
    "train_df = train_df.repartition(100)\n",
    "test_df = test_df.repartition(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from sparkdl import DeepImageFeaturizer\n",
    "\n",
    "featurizer = DeepImageFeaturizer(inputCol=\"image\", outputCol=\"features\", modelName=\"InceptionV3\")\n",
    "lr = LogisticRegression(maxIter=20, regParam=0.05, elasticNetParam=0.3, labelCol=\"label\")\n",
    "p = Pipeline(stages=[featurizer, lr])\n",
    "\n",
    "p_model = p.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "tested_df = p_model.transform(test_df)\n",
    "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "print(\"Test set accuracy = \" + str(evaluator.evaluate(tested_df.select(\"prediction\", \"label\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "def _p1(v):\n",
    "  return float(v.array[1])\n",
    "\n",
    "p1 = udf(_p1, DoubleType())\n",
    "df = tested_df.withColumn(\"p_1\", p1(tested_df.probability))\n",
    "wrong_df = df.orderBy(expr(\"abs(p_1 - label)\"), ascending=False)\n",
    "wrong_df.select(\"image.origin\", \"p_1\", \"label\").limit(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.image import ImageSchema\n",
    "from sparkdl import DeepImagePredictor\n",
    "\n",
    "# 빠른 테스트를 위해 앞서 선언한 샘플 이미지 디렉터리를 사용합니다.\n",
    "image_df = ImageSchema.readImages(sample_img_dir)\n",
    "\n",
    "predictor = DeepImagePredictor(inputCol=\"image\", outputCol=\"predicted_labels\", modelName=\"InceptionV3\", decodePredictions=True, topK=10)\n",
    "predictions_df = predictor.transform(image_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = p_model.transform(image_df)\n",
    "df.select(\"image.origin\", (1-p1(df.probability)).alias(\"p_daisy\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import InceptionV3\n",
    "from sparkdl.udf.keras_image_model import registerKerasImageUDF\n",
    "from keras.applications import InceptionV3\n",
    "\n",
    "registerKerasImageUDF(\"my_keras_inception_udf\", InceptionV3(weights=\"imagenet\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
